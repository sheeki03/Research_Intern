# Notion â†’ AI Research Pipeline

Automates the full research workflow for crypto projects stored in a **Notion** CRM or Cards Database:

1. Detect a card whose "Due-Diligence Questionnaire" (DDQ) child-page has been marked as **Completed**.
2. Generate an in-depth Markdown report via the multi-step *Deep Research* agent (web scraping + LLM).
3. Publish/overwrite a child page called **"AI Deep Research Report"** directly under the Notion card.
4. Run an LLM-based scoring function to rate the project and save the JSON answers.
5. Publish a ğŸ”¥ **Ratings** inline database with nested cards to relay project scores and Q&A, also for analysts to consult, chime in on Notion.

All steps run entirely server-less â€“ schedule `python main.py` on a cron / GitHub Actions and you are done.

---

## ğŸ–¼ï¸ Pipeline Overview

```mermaid
flowchart TD
    A[watcher.py poll Notion DB] -->|Completed DDQ| B[research.py deep-web research]
    B --> C[writer.py create/overwrite report page]
    C --> D[scorer.py LLM JSON scoring]
    D --> E[pusher.py update ğŸ”¥ Ratings DB]
```

Every module can be executed independently (useful during development) yet `main.py` stitches them together for weekly automation.

---

## ğŸ—„ï¸ Repo Layout

```
.
â”œâ”€â”€ src/                # Runtime modules (watcher, research, writer, scorer, pusher)
â”œâ”€â”€ web_research/       # Deep-research agent & async search/scrape stack
â”œâ”€â”€ tests/              # Pytest suite covering the whole flow
â”œâ”€â”€ main.py             # Orchestrator (weekly cron job)
â”œâ”€â”€ requirements.txt    # Pinned dependencies
â””â”€â”€ README.md           # â† you are here
```

---

## âš¡ Quick Start

### 1. Clone & enter
```bash
git clone https://github.com/Liscivia/AI_intern.git
cd AI_intern
```

### 2. Create a virtual environment (Python 3.11)
**Windows PowerShell**
```powershell
python -m venv ai_intern
.\ai_intern\Scripts\Activate.ps1
```
**macOS / Linux**
```bash
python3 -m venv ai_intern
source ai_intern/bin/activate
```

### 3. Install dependencies
```bash
pip install --upgrade pip
pip install -r requirements.txt
# Playwright needs browser binaries once
playwright install
```

### 4. Environment variables
Create the `.env` file, using the `.env.example` as reference:
```dotenv
# Notion
NOTION_TOKEN=secret_â€¦
NOTION_DB_ID=<database-id> # enable automation on the notion CRM itself 

# LLM provider
OPENAI_API_KEY=sk-â€¦
OPENAI_MODEL=gpt-4o-mini    # or any compatible model (deep-thinking & strong tool-calling capabilities recommended)

# Web-scraping
#   "firecrawl" (API-based) is default â€“ set key or flip to Playwright fallback.
FIRECRAWL_API_KEY=fc_â€¦       # recommended
DEFAULT_SCRAPER=firecrawl    # or playwright_ddgs

# Prompts
DEEP_RESEARCH_PROMPT="You areâ€¦"
```

### 5. Run the pipeline once
```bash
python main.py
```
Or execute the full test that mirrors the cron job:
```bash
pytest tests/test_final.py -q
```
or execute any of the tests to check out the functioning of each components
```bash
pytest tests/test_watcher.py -v -s
```

---

## ğŸ“ Logs

File | Purpose
---- | -------
`logs/watcher.log` | Notion polling & pagination
`logs/research.log` | Deep-research orchestration & web searches
`logs/writer.log` | Markdown â†’ Notion block conversion
`logs/scorer.log` | LLM scoring lifecycle

Each line is written in `key=value` format so you can grep/filter easily.

---

## ğŸª„ Troubleshooting

| Symptom | Fix |
| ------- | --- |
| `RuntimeError: Environment variable NOTION_TOKEN is required.` | Load/define all required vars (`NOTION_TOKEN`, `NOTION_DB_ID`, `OPENAI_API_KEY`). |
| Firecrawl 429s / quota | Lower `RESEARCH_CONCURRENCY` env vars or set `DEFAULT_SCRAPER=playwright_ddgs`. |




